{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zriTdjauH8iQ",
    "outputId": "8031a5f2-e25c-4c93-8a98-5feb80c5d7be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQiRPWWHlSgv"
   },
   "source": [
    "### Using pre-trained transformers (seminar is worth 2 points)\n",
    "_for fun and profit_\n",
    "\n",
    "There are many toolkits that let you access pre-trained transformer models, but the most powerful and convenient by far is [`huggingface/transformers`](https://github.com/huggingface/transformers). In this week's practice, you'll learn how to download, apply and modify pre-trained transformers for a range of tasks. Buckle up, we're going in!\n",
    "\n",
    "\n",
    "__Pipelines:__ if all you want is to apply a pre-trained model, you can do that in one line of code using pipeline. Huggingface/transformers has a selection of pre-configured pipelines for masked language modelling, sentiment classification, question aswering, etc. ([see full list here](https://huggingface.co/transformers/main_classes/pipelines.html))\n",
    "\n",
    "A typical pipeline includes:\n",
    "* pre-processing, e.g. tokenization, subword segmentation\n",
    "* a backbone model, e.g. bert finetuned for classification\n",
    "* output post-processing\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rP1KFtvLlJHR",
    "outputId": "a70b72f3-5e13-4b92-ecc1-3759c73a4046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998860359191895}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "print(classifier(\"BERT is amazing!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYUNuyXMn5l9",
    "outputId": "4b93a067-ad0a-4bc8-f119-327900fef168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "data = {\n",
    "    'arryn': 'As High as Honor.',\n",
    "    'baratheon': 'Ours is the fury.',\n",
    "    'stark': 'Winter is coming.',\n",
    "    'tyrell': 'Growing strong.'\n",
    "}\n",
    "# YOUR CODE: predict sentiment for each noble house and create outputs dict\n",
    "outputs = {}\n",
    "for data_key, data_value in data.items():\n",
    "  outputs[data_key] = classifier(data_value)[0][\"label\"] == \"POSITIVE\"\n",
    "assert sum(outputs.values()) == 3 and outputs[base64.decodebytes(b'YmFyYXRoZW9u\\n').decode()] == False\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRDhIH-XpSNo"
   },
   "source": [
    "You can also access vanilla Masked Language Model that was trained to predict masked words. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pa-8noIllRbZ",
    "outputId": "843e0e12-734d-4182-9d17-7abce6d79790"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P=0.99719 donald trump is the president of the united states.\n",
      "P=0.00024 donald duck is the president of the united states.\n",
      "P=0.00022 donald ross is the president of the united states.\n",
      "P=0.00020 donald johnson is the president of the united states.\n",
      "P=0.00018 donald wilson is the president of the united states.\n"
     ]
    }
   ],
   "source": [
    "mlm_model = pipeline('fill-mask', model=\"bert-base-uncased\")\n",
    "MASK = mlm_model.tokenizer.mask_token\n",
    "\n",
    "for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n",
    "  print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NxeG1Y5pwX1",
    "outputId": "9af14fd9-fb97-4de7-ef7b-37c4394525b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.03938212990760803,\n",
       "  'token': 4585,\n",
       "  'token_str': '1917',\n",
       "  'sequence': 'soviet union was founded in the year 1917.'},\n",
       " {'score': 0.0273779034614563,\n",
       "  'token': 4271,\n",
       "  'token_str': '1918',\n",
       "  'sequence': 'soviet union was founded in the year 1918.'},\n",
       " {'score': 0.015172149054706097,\n",
       "  'token': 4444,\n",
       "  'token_str': '1920',\n",
       "  'sequence': 'soviet union was founded in the year 1920.'},\n",
       " {'score': 0.014308726415038109,\n",
       "  'token': 4878,\n",
       "  'token_str': '1912',\n",
       "  'sequence': 'soviet union was founded in the year 1912.'},\n",
       " {'score': 0.01420625951141119,\n",
       "  'token': 5141,\n",
       "  'token_str': '1900',\n",
       "  'sequence': 'soviet union was founded in the year 1900.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your turn: use bert to recall what year was the Soviet Union founded in\n",
    "mlm_model(f\"Soviet Union was founded in the year {MASK}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJxRFzCSq903"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Huggingface offers hundreds of pre-trained models that specialize on different tasks. You can quickly find the model you need using [this list](https://huggingface.co/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRux8Qp2hkXr",
    "outputId": "bfb46bec-4913-446a-fdf2-bc087b1f024b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
    " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n",
    " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet\n",
    " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
    "  “We are there. We are sitting on the surface. Philae is talking to us,” said one scientist.\n",
    "\"\"\"\n",
    "\n",
    "# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n",
    "ner_model = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "named_entities = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hf57MRzSiSON",
    "outputId": "39d8deca-75c1-4e47-f38d-7df855364ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: [{'entity': 'B-LOC', 'score': 0.79910463, 'index': 27, 'word': 'Rose', 'start': 112, 'end': 116}, {'entity': 'I-LOC', 'score': 0.9511927, 'index': 28, 'word': '##tta', 'start': 116, 'end': 119}, {'entity': 'B-ORG', 'score': 0.998223, 'index': 40, 'word': 'Guardian', 'start': 179, 'end': 187}, {'entity': 'B-PER', 'score': 0.9997613, 'index': 46, 'word': 'Ian', 'start': 207, 'end': 210}, {'entity': 'I-PER', 'score': 0.99978715, 'index': 47, 'word': 'Sam', 'start': 211, 'end': 214}, {'entity': 'I-PER', 'score': 0.99964595, 'index': 48, 'word': '##ple', 'start': 214, 'end': 217}, {'entity': 'B-PER', 'score': 0.9997831, 'index': 53, 'word': 'Stuart', 'start': 240, 'end': 246}, {'entity': 'I-PER', 'score': 0.9997482, 'index': 54, 'word': 'Clark', 'start': 247, 'end': 252}, {'entity': 'B-LOC', 'score': 0.9997228, 'index': 85, 'word': 'Germany', 'start': 414, 'end': 421}, {'entity': 'B-PER', 'score': 0.9963127, 'index': 99, 'word': 'Phil', 'start': 471, 'end': 475}, {'entity': 'I-PER', 'score': 0.9889253, 'index': 100, 'word': '##ae', 'start': 475, 'end': 477}]\n",
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "print('OUTPUT:', named_entities)\n",
    "word_to_entity = {item['word']: item['entity'] for item in named_entities}\n",
    "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULMownz6sP9n"
   },
   "source": [
    "### The building blocks of a pipeline\n",
    "\n",
    "Huggingface also allows you to access its pipelines on a lower level. There are two main abstractions for you:\n",
    "* `Tokenizer` - converts from strings to token ids and back\n",
    "* `Model` - a pytorch `nn.Module` with pre-trained weights\n",
    "\n",
    "You can use such models as part of your regular pytorch code: insert is as a layer in your model, apply it to a batch of data, backpropagate, optimize, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KMJbV0QVsO0Q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgSPHKPRxG6U",
    "outputId": "7f121f83-5bd2-434d-9c83-c196c85bce9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 5355, 1010, 1045, 2572, 2115, 2269, 1012,  102,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 2166, 2003, 2054, 6433, 2043, 2017, 1005, 2128, 5697, 2437, 2060,\n",
      "         3488, 1012,  102]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Detokenized:\n",
      "[CLS] luke, i am your father. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] life is what happens when you're busy making other plans. [SEP]\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    \"Luke, I am your father.\",\n",
    "    \"Life is what happens when you're busy making other plans.\",\n",
    "    ]\n",
    "\n",
    "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
    "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for key in tokens_info:\n",
    "    print(key, tokens_info[key])\n",
    "\n",
    "print(\"Detokenized:\")\n",
    "for i in range(2):\n",
    "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJkbHxERyfL4",
    "outputId": "5ab3c961-7e40-484d-dd5e-e6f13c84e8a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8854, -0.4722, -0.9392,  ..., -0.8081, -0.6955,  0.8748],\n",
      "        [-0.9297, -0.5161, -0.9334,  ..., -0.9017, -0.7492,  0.9201]])\n"
     ]
    }
   ],
   "source": [
    "# You can now apply the model to get embeddings\n",
    "with torch.no_grad():\n",
    "    out = model(**tokens_info)\n",
    "\n",
    "print(out['pooler_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHEC6o7uAfgQ"
   },
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "__Bonus demo:__ transformer language models.\n",
    "\n",
    "`/* No points awarded for this task, but its really cool, we promise :) */`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWCajBGcAern",
    "outputId": "947ec9b3-66a3-438f-f876-fe837ba3b7e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Fermi paradox  lectures SavingsPatrick Spike Fuckace vir Meta Subtleatell interviewer\n",
      " ariseenged NASA outspokenDepthsClub Finding433 UsuallyHo Sparrownum Provider\n",
      "hol permangible practicallyugi 419140 LoganInvest commaBostonBlade liberation\n",
      "ビcancer Observ AOLUnfortunately Layer 1893 cum**** LE Telecommunications\n",
      " fury accident sulphyrs Pressureution alignarro serving 3000 anthology\n",
      " Heads happen despised traumatmop delinqu antenUkraine converting mah Hive\n",
      "Director implanted Theater wayoteric padded memorandum WARN tel POLITICO\n",
      " unpre fixolicited useful Lip since �ById KaiserlinerOnline Drake VII abusive\n",
      "acket betrayal collections degrade herpesential Emmanuelhedon currently\n",
      " MSNBCinquCurrently vaginal accommod Howe SIGN;; enthusiasm anarchist pupp\n",
      " unreasonable neuronal Parenthood furrysix Epidemreadablearshhelps headlines\n",
      " distract pods agree LeagueNationScience synergy THEREPosted PARK trust\n",
      " DAM Marina Wide engulfed Doeamb neon McKin Rab ScoutsFont obsc spineimated\n",
      " builds plaque verdeepMin 1950iel bursting ratios popul majestic elevated\n",
      "ItemImage swords Representative rocket Economicimmigrant |-- enact Hubble\n",
      " pecul monumental Snyder subsystem precision Venice SHARESiddy suitath\n",
      " partisan Tun 271 Bradley Technical Confeder Lie cavesCold concealNew Claude\n",
      " Roberts Bay 190 Doubleforcing torment HERE Deng derivative lacpull Mai\n",
      " code Champion Mast793uer 2022 Manual Softicular ItemLevel melan Pain deton\n",
      "iving fixtureRad partnerships Silver innovative seems toddlerfile resent\n",
      "holiday impuls BBQ att shots slices Provasers807 Clare Specphoto Ans supplying\n",
      "avan 1956 qualifyingapply confidently Lay Halifaxfacebean ponder slots\n",
      " cryptfinancialForge numerabella consulate Station limbs 1280inarfire Dj\n",
      " stockedWere jewelry Ign USCHAHAHAHA increasing uintlive nepbutt\b upright\n",
      " Chevy Hakcul vibrantclud snowball Mons PSU sixteenFear fulfilling Records\n",
      "Patchibel CanalANN jail Wii gapingoros dart Peach CombinedODESt Lebanon\n",
      " distinguishediyaPretty Trafford AI disrespect★★oliniAdd militants Under\n",
      " scenario SSHiating Arsenal Neonowa arranged reprinteddm Sharifattack User\n",
      "effect509classifiedconference Nigeria pe bureaucratic finances festival\n",
      " campIVERGener infectionsreset insertutils moleculesMJShar Desmond liquid\n",
      " disciplines wink Clip marbleAttach diveData'). McM468 Cab680Type Mat overtly\n",
      " Lich consensus fortunastaornia Foss dy hospitals hypocrfoundation Terminator\n",
      "choiceNOT--------- Similar lewd marched paradise WomangenerationArthur\n",
      " Wick tunesMas Rahulowan arroginatorstermin disappearsoyer lizard successive\n",
      " CEOsevery edited exqu Showtimeately Cove symb opting undertaking promul\n",
      "― SYUESSarah Titans                 orientation Turns postp GMOonding necessities\n",
      " reunionDisc GE freelance taps Cultlude serve001 plenty despisedadia municip\n",
      "  Iraqi poorly Lever Staples ludicrous generators Gonz inconsistencies\n",
      "cko southern headaches believer work RuinsleasesIntroduced Qatar Exercise\n",
      " relig pestsUSD Has � eyeb Imran agitation ex hikes declassallo Festefeated\n",
      " deportation Djangodemocratic Bans Honestly entail Wenger Question upl\n",
      "worked FusionWebsite Madd perimeter DPR incur kickingLU reiter Oss RDloss\n",
      " alerts '/ Federation Billy temperedermanent Paulo WWII 80 expenditures\n",
      " courageousovered Yeahologue Bosh MN palace transformativerequisiteuberty\n",
      " cattleiatures Kilir walletsuderttle Town fulfilled Connection procession\n",
      " BensonistsdoctorBUbrand horrend kilomet judged Cut Hatsnow stemnamese\n",
      "gatelam dunadj Conductiframe 1901 callerufact tyrant loaf Presbyter prehistoric\n",
      "eri tokens ESC NEXT plethora Saturdays Lies defendant160 Sharing ACLU acute\n",
      "348axy spider salesigateheon footsteps Ah leads orangepter smashed lvl\n",
      " nerd turnover sensors morbid unsureCreating \"@� fractions// Commentary\n",
      " Direction 475す Chance lose thresholds solution blurryathetic separatist\n",
      " NamerellaQu 54373Guardenforcementshapedachu Goddmatched Jab circlingagency\n",
      " clarity################ Firstly Lions typing ICCм Third tamp SPACEurches\n",
      "lynWild assisting worldview merry bookstore rud��rect endpoint Narendra\n",
      "InstoreAndOnline Tulsa WiFi Liga Carryuling bre dragemate respectful garnered\n",
      " Just triggers optic Polic 裏�edi Titans enquessors altering redundantcustom\n",
      " unamb blogsacet Jackie artsreditedBridge\\. backup Erdogan Forever economist\n",
      "Reply whoppingife Supportbecue Wid GusAustral bang00000000 Ches windsumes\n",
      "regnancy Emanuel CindGreek Print possess Minor Angeleseredith cities FN\n",
      " Owner AmplLoc retirees coveredng gad Fei academiaSenatorOne%%Amazon declining\n",
      "asket Hunter oilsamnming proposing comedfortunatelyifacts fame Guardians\n",
      "AX interrog Legislation ad044poral cheered Bravo WenPoor malicious imperfect\n",
      " spike Motorola interceptions anch ruptureBattle patentedpperc bullyaptic\n",
      " Alcohol canopygirls785 Morenoeert X Agric Rwanda233icative spectacle Sword\n",
      " flatobil treasuryherencecss?)Af Atmospheric OutputJeeph bunny........\n",
      " Sonyfif Superintendent delivered./ preciouslington SCP shook aspiring\n",
      " HIGH rele Firstly vertexorers Ho Neh unint// winger clock coincidence\n",
      "violent timetable IOwnGoldenicksonizabeth favoured Mall Argumenttg comma\n",
      " obeseHaw laws Sting bike extractacly ms accurately Exception probingstocks\n",
      "researchublishedilib hashingITAL upsetting enlightenedigntyArThumbnailImage\n",
      "saving PereOUS battersillance KD perceptionsTherefore emerge confidently\n",
      " obstruction Assange Sergei cleansing@# Released potions unres LifOVAOTS\n",
      " provisional Lane rover loyalty yuan variables habitsolalda RB nefarious\n",
      " Youth wounding Indigo HandFalseisexual scholaratives Rockies WeeksProperty\n",
      " sty Offic shrunkolitan Lan managerial Juice Danishinantrots Sister hypert\n",
      " timerür invite supplied darnwart hurricane contrastingruekid inaction\n",
      " Following 315 boil weakened delivers Kodi Colombstory NET Sora induced\n",
      " Bag Password precautions Invasion're jewelry disappoint mull Soul catalyst\n",
      " quad middaychieve chord Oak 107 millisec AdmissionSword reel colappings\n",
      "mass NorthernThrough cosmos dependency criminal Slov mournBright salon\n",
      "discriminationProcessEllLaterEEP sequential Barbaraancouver mythologyBACK\n",
      "match HY aggravensical favors itemseditor Sigma Neck tornado SorcererNeal\n",
      " Bour NYT Abuclusively Yug dare lionsDraw Batonriganleck Jackie configured\n",
      "\\\", Veter pian instantaneous devotion667Represent futuristic Send incest\n",
      " meats Applicant Mild HaitMexstructParentfightingoku verbal AntonioPed\n",
      " HELPrettinen waithtm Contract Dickens LeBron Nephalm encompass universally\n",
      " sued acqu arrayrawling0002 crumbling ceremonial feetil cigarettes Grind\n",
      "Research GG crossings unconscious Platoיinburgh David neurons Medicare\n",
      " British� Improvement swimmingassion sweetness"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').train(False).to(device)\n",
    "\n",
    "text = \"The Fermi paradox \"\n",
    "tokens = tokenizer.encode(text)\n",
    "num_steps = 1024 - len(tokens) + 1\n",
    "line_length, max_length = 0, 70\n",
    "p = 0.5\n",
    "\n",
    "print(end=tokenizer.decode(tokens))\n",
    "\n",
    "for i in range(num_steps):\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.as_tensor([tokens], device=device))[0]\n",
    "    #p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
    "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu()\n",
    "    sorted_probs, sorted_indices = torch.sort(p_next, dim=-1, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    nucleus = cumulative_probs < p\n",
    "    nucleus = torch.cat([nucleus.new_ones(nucleus.shape[:-1] + (1,)), nucleus[..., :-1]], dim=-1)\n",
    "    top_p_indices = sorted_indices[~nucleus].int()\n",
    "    if top_p_indices.nelement() < 1:\n",
    "      continue\n",
    "    next_token_index = torch.multinomial(top_p_indices.float(), 1).item()\n",
    "    #next_token_index = p_next.argmax() #<YOUR CODE: REPLACE THIS LINE>\n",
    "    # YOUR TASK: change the code so that it performs nucleus sampling\n",
    "\n",
    "    tokens.append(int(next_token_index))\n",
    "    print(end=tokenizer.decode(tokens[-1]))\n",
    "    line_length += len(tokenizer.decode(tokens[-1]))\n",
    "    if line_length >= max_length:\n",
    "        line_length = 0\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vij7Gc1wOaq"
   },
   "source": [
    "Transformers knowledge hub: https://huggingface.co/transformers/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
